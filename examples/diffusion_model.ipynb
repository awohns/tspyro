{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example geography with two islands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the accuracy of the waypoints model using two island simulation and hex-grid waypoints.\n",
    "\n",
    "We'll use the real times of ancestral nodes in the graph so that we only have to infer the location of ancestors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from tspyro.diffusion import make_hex_grid\n",
    "import tspyro\n",
    "import tsdate\n",
    "import pyslim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tskit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw islands\n",
    "Create the island model, output greyscale for SLiMGui to read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables for Islands Geography\n",
    "bounds = dict(west=0, east=4.0, south=0, north=2.24)\n",
    "island_center = [(1.33, 1.2), (2.66, 1.2)]\n",
    "island_radius = [0.33, 0.33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle1 = plt.Circle(island_center[0], island_radius[0] * 2, color='black')\n",
    "circle2 = plt.Circle(island_center[1], island_radius[1] * 2, color='black')\n",
    "\n",
    "fig, ax = plt.subplots() # note we must use plt.subplots, not plt.subplot\n",
    "# (or if you have an existing figure)\n",
    "# fig = plt.gcf()\n",
    "# ax = fig.gca()\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 2.4)\n",
    "\n",
    "ax.add_patch(circle1)\n",
    "ax.add_patch(circle2)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"close_islands.png\")\n",
    "from PIL import Image\n",
    "Image.open('close_islands.png').convert('L').save('close_islands.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pyslim.load(\"close_islands.trees\").simplify()\n",
    "recap_ts = ts.recapitate(recombination_rate=1e-8, Ne=50).simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the real locations of nodes\n",
    "\n",
    "lat_long = []\n",
    "for node in recap_ts.nodes():\n",
    "    if node.individual != -1:\n",
    "        ind = recap_ts.individual(node.individual)\n",
    "    \n",
    "        lat_long.append([ind.location[0], ind.location[1]])\n",
    "    else:\n",
    "        print(node)\n",
    "lat_long = np.array(lat_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the real locations of nodes\n",
    "plt.scatter(lat_long[:,0], lat_long[:,1], s=0.1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match waypoints to model from above\n",
    "grid_radius = 0.1\n",
    "\n",
    "def on_land(x, y):\n",
    "    result = torch.tensor(False)\n",
    "    for (x0, y0), r in zip(island_center, island_radius):\n",
    "        result = result | (r > (x - x0) ** 2 + (y - y0) ** 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_hex_grid(**bounds, radius=grid_radius, predicate=on_land)\n",
    "waypoints = grid[\"waypoints\"]\n",
    "transition = grid[\"transition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Probability of transitioning from a given point\")\n",
    "plt.scatter(waypoints[:, 0], waypoints[:, 1], c=transition[0])\n",
    "plt.xlim(bounds[\"west\"], bounds[\"east\"])\n",
    "plt.ylim(bounds[\"south\"], bounds[\"north\"])\n",
    "# Add islands in background\n",
    "circle1 = plt.Circle(island_center[0], island_radius[0] * 2, color='black', alpha=0.1, zorder=-1)\n",
    "circle2 = plt.Circle(island_center[1], island_radius[1] * 2, color='black', alpha=0.1, zorder=-1)\n",
    "plt.gca().add_patch(circle1)\n",
    "plt.gca().add_patch(circle2)\n",
    "\n",
    "# plt.axis(\"equal\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And note the spread of lineages is roughly covered by the waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(waypoints[:, 0], waypoints[:, 1])\n",
    "plt.xlim(bounds[\"west\"], bounds[\"east\"])\n",
    "plt.ylim(bounds[\"south\"], bounds[\"north\"])\n",
    "circle1 = plt.Circle(island_center[0], island_radius[0] * 2, color='black', alpha=0.1, zorder=-1)\n",
    "circle2 = plt.Circle(island_center[1], island_radius[1] * 2, color='black', alpha=0.1, zorder=-1)\n",
    "plt.gca().add_patch(circle1)\n",
    "plt.gca().add_patch(circle2)\n",
    "plt.tight_layout()\n",
    "plt.scatter(lat_long[:,0], lat_long[:,1], s=0.1, label=\"real location of ancestors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let's find a lineage that \"jumps\" between islands.\n",
    "But curiously, there don't seem to be any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: could tune distance between two islands until see jumpes\n",
    "migrants = set()\n",
    "for tree in recap_ts.trees():\n",
    "    for node in recap_ts.samples():\n",
    "        locs = []\n",
    "        while node != -1 and node < ts.num_nodes:\n",
    "            locs.append(lat_long[node])\n",
    "            node = tree.parent(node)\n",
    "        locs = np.array(locs)\n",
    "        if np.any(locs[:,0] < 2) and np.any(locs[:,0] > 2):\n",
    "            migrants.add(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to infer the location of ancestors.\n",
    "\n",
    "The following code is messy, but some details are still to be worked out.\n",
    "Obviously we'll want a simple interface for users to choose the browning motion model or waypoint model, and will want to implement this without duplicating code. Q: how should we organise the code for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer import Trace_ELBO, SVI\n",
    "from pyro.infer.autoguide import AutoDelta, AutoNormal\n",
    "from pyro.optim import Adam\n",
    "from pyro.nn import PyroModule\n",
    "from pyro.infer.reparam import LocScaleReparam\n",
    "\n",
    "from pyro.distributions import constraints, TorchDistribution\n",
    "from torch.distributions.utils import broadcast_all\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "\n",
    "torch.set_default_dtype(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NaiveModel(PyroModule):\n",
    "    def __init__(self, ts, *, Ne, prior, transitions, waypoints, waypoint_radius,\n",
    "               node_times=None, leaf_location=None, mutation_rate=1e-8, penalty=100.):\n",
    "        super().__init__()\n",
    "        nodes = ts.tables.nodes\n",
    "        edges = ts.tables.edges\n",
    "        self.is_leaf = torch.tensor((nodes.flags & 1).astype(bool), dtype=torch.bool)\n",
    "        self.is_internal = ~self.is_leaf\n",
    "        self.num_nodes = len(self.is_leaf)\n",
    "        self.num_internal = self.is_internal.sum().item()\n",
    "\n",
    "        self.ts = ts\n",
    "\n",
    "        self.parent = torch.tensor(edges.parent, dtype=torch.long)\n",
    "        self.child = torch.tensor(edges.child, dtype=torch.long)\n",
    "        self.span = torch.tensor(edges.right - edges.left, dtype=torch.float)\n",
    "        self.mutations = torch.tensor(self.get_mut_edges(), dtype=torch.float) # this is an int, but we optimise with float for pytorch\n",
    "\n",
    "        self.penalty = float(penalty)\n",
    "        self.Ne = float(Ne)\n",
    "        self.mutation_rate = mutation_rate\n",
    "\n",
    "        self.node_times=node_times\n",
    "\n",
    "        # conditional coalescent prior\n",
    "        timepoints = torch.as_tensor(prior.timepoints, dtype=torch.float)\n",
    "        timepoints = timepoints.mul(2*Ne).log1p()\n",
    "        # timepoints = timepoints.clamp(min=0)\n",
    "        grid_data = torch.as_tensor(prior.grid_data[:], dtype=torch.float)\n",
    "        grid_data = grid_data / grid_data.sum(1, True)\n",
    "        self.prior_loc = torch.einsum(\"t,nt->n\", timepoints, grid_data)\n",
    "        deltas = (timepoints - self.prior_loc[:,None]) ** 2\n",
    "        self.prior_scale = torch.einsum(\"nt,nt->n\", deltas, grid_data).sqrt()\n",
    "        # self.prior_loc += math.log(Ne)\n",
    "\n",
    "        # GEOGRAPHY\n",
    "        self.leaf_location = leaf_location\n",
    "        # Pass in the transition matrix and waypoints\n",
    "        self.transition = transition\n",
    "        self.waypoint_radius = waypoint_radius\n",
    "        self.waypoints = waypoints\n",
    "        # This cheaply precomputes some matrices.\n",
    "        self.matrix_exp = tspyro.diffusion.ApproximateMatrixExponential(self.transition)\n",
    "\n",
    "    def get_mut_edges(self):\n",
    "        \"\"\"\n",
    "        Get the number of mutations on each edge in the tree sequence.\n",
    "        \"\"\"\n",
    "        ts = self.ts\n",
    "        edge_diff_iter = ts.edge_diffs()\n",
    "        right = 0\n",
    "        edges_by_child = {}  # contains {child_node:edge_id}\n",
    "        mut_edges = np.zeros(ts.num_edges, dtype=np.int64)\n",
    "        for site in ts.sites():\n",
    "            while right <= site.position:\n",
    "                (left, right), edges_out, edges_in = next(edge_diff_iter)\n",
    "                for e in edges_out:\n",
    "                    del edges_by_child[e.child]\n",
    "                for e in edges_in:\n",
    "                    assert e.child not in edges_by_child\n",
    "                    edges_by_child[e.child] = e.id\n",
    "            for m in site.mutations:\n",
    "                # In some cases, mutations occur above the root\n",
    "                # These don't provide any information for the inside step\n",
    "                if m.node in edges_by_child:\n",
    "                    edge_id = edges_by_child[m.node]\n",
    "                    mut_edges[edge_id] += 1\n",
    "        return mut_edges\n",
    "\n",
    "    def forward(self):\n",
    "        # First sample times from an improper uniform distribution which we denote\n",
    "        # via .mask(False). Note only the internal nodes are sampled; leaves are\n",
    "        # fixed at zero.\n",
    "        # Note this isn't a coalescent prior, but some are available at:\n",
    "        # https://docs.pyro.ai/en/stable/_modules/pyro/distributions/coalescent.html\n",
    "        with pyro.plate(\"internal_nodes\", self.num_internal):\n",
    "            if self.node_times is None:\n",
    "                internal_time = pyro.sample(\n",
    "                \"internal_time\",\n",
    "                # dist.Exponential(1).mask(False),\n",
    "                dist.LogNormal(self.prior_loc, self.prior_scale).mask(True) # False turns off prior but uses it for initialisation\n",
    "                ) # internal time is modelled in logspace\n",
    "            else:\n",
    "                internal_time = self.node_times[self.ts.num_samples:]\n",
    "            # GEOGRAPHY.\n",
    "            # Sample location from a flat prior, we'll add a pyro.factor statement later.\n",
    "            internal_location = pyro.sample(\n",
    "            \"internal_location\",\n",
    "            dist.Normal(torch.zeros(2), torch.ones(2)).to_event(1).mask(False),\n",
    "            )\n",
    "\n",
    "        internal_time = internal_time\n",
    "        time = torch.zeros((self.num_nodes,))\n",
    "        time[self.is_internal] = internal_time\n",
    "        # GEOGRAPHY.\n",
    "        # Should we be Bayesian about migration scale, or should it be fixed?\n",
    "        migration_scale = pyro.sample(\"migration_scale\", dist.LogNormal(-5, 1))\n",
    "\n",
    "        # Next add a factor for time gaps between parents and children.\n",
    "        gap = time[self.parent] - time[self.child]\n",
    "\n",
    "        with pyro.plate(\"edges\", len(gap)):\n",
    "            if self.node_times is None:\n",
    "                rate = (gap * self.span * self.mutation_rate).clamp(min=1e-8)\n",
    "                pyro.sample(\n",
    "                \"mutations\",\n",
    "                dist.Poisson(rate),\n",
    "                obs=self.mutations,\n",
    "                )\n",
    "            else:\n",
    "                time = self.node_times\n",
    "            if self.leaf_location is not None:\n",
    "                gap = gap.clamp(min=1e-10)\n",
    "                # GEOGRAPHY.\n",
    "                # The following encodes that children migrate away from their parents\n",
    "                # following brownian motion with rate migration_scale.\n",
    "                location = torch.cat([self.leaf_location, internal_location], 0)\n",
    "                parent_location = location.index_select(0, self.parent)\n",
    "                child_location = location.index_select(0, self.child)\n",
    "\n",
    "                pyro.sample(\n",
    "                    \"migration\",\n",
    "                    tspyro.diffusion.WaypointDiffusion2D(\n",
    "                        source=parent_location,\n",
    "                        time=gap,\n",
    "                        radius=self.waypoint_radius,\n",
    "                        waypoints=self.waypoints,\n",
    "                        matrix_exp=self.matrix_exp,\n",
    "                    ),\n",
    "                    obs=child_location,\n",
    "                )\n",
    "            else:\n",
    "                location = torch.ones(self.ts.num_nodes)\n",
    "        return time, gap, location, migration_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReparamModel(NaiveModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        ts = self.ts\n",
    "        dtype = torch.zeros(()).dtype  # grab the default dtype for ts->torch conversion\n",
    "        D = 2  # assume 2D for now\n",
    "        N = ts.num_nodes\n",
    "        L = len(self.leaf_location)\n",
    "    \n",
    "        # These are coefficients in an affine reparametrization:\n",
    "        #   location = baseline_0 + baseline_1 @ delta  # i.e. constant + matmul\n",
    "        # where the delta matrix of shape (N,D) is our new latent variable. \n",
    "        baseline_0 = torch.zeros(N, D)  # intercept\n",
    "        baseline_0[:L] = self.leaf_location\n",
    "        baseline_1 = torch.zeros(N, N)  # slope\n",
    "        baseline_1[L:, L:] = torch.eye(N - L)  # each location depends on its delta\n",
    "        times = torch.tensor(ts.tables.nodes.time, dtype=dtype)  # assume times are fixed\n",
    "        \n",
    "        for parent, edges in itertools.groupby(ts.edges(), operator.attrgetter(\"parent\")):\n",
    "            children = [e.child for e in edges]\n",
    "            assert children\n",
    "            children = torch.tensor(children)\n",
    "            \n",
    "            # Compute a 1/gap weighted average of child locations.\n",
    "            gaps = times[parent] - times[children]\n",
    "            assert (gaps > 0).all()\n",
    "            weights = 1 / gaps  # Brownian weights\n",
    "            weights /= weights.sum()  # normalize\n",
    "        \n",
    "            # Parent's location is weighted sum of child locations, plus delta.\n",
    "            baseline_0[parent] = (weights[:, None] * baseline_0[children]).sum(0)\n",
    "            baseline_1[parent] += (weights[:, None] * baseline_1[children]).sum(0)\n",
    "        # Restrict to internal nodes.\n",
    "        baseline_0 = baseline_0[L:].clone()\n",
    "        baseline_1 = baseline_1[L:, L:].clone()\n",
    "        self.baseline = baseline_0, baseline_1\n",
    "\n",
    "    def forward(self):\n",
    "        # First sample times from an improper uniform distribution which we denote\n",
    "        # via .mask(False). Note only the internal nodes are sampled; leaves are\n",
    "        # fixed at zero.\n",
    "        # Note this isn't a coalescent prior, but some are available at:\n",
    "        # https://docs.pyro.ai/en/stable/_modules/pyro/distributions/coalescent.html\n",
    "        with pyro.plate(\"internal_nodes\", self.num_internal):\n",
    "            if self.node_times is None:\n",
    "                internal_time = pyro.sample(\n",
    "                \"internal_time\",\n",
    "                # dist.Exponential(1).mask(False),\n",
    "                dist.LogNormal(self.prior_loc, self.prior_scale).mask(True) # False turns off prior but uses it for initialisation\n",
    "                ) # internal time is modelled in logspace\n",
    "            else:\n",
    "                internal_time = self.node_times[self.ts.num_samples:]\n",
    "            # GEOGRAPHY.\n",
    "            # Locations will be parametrized as difference from a baseline, where the\n",
    "            # baseline location of a parent is a weighted sum of child locations.\n",
    "            # Sample locations from a flat prior, we'll add a pyro.factor statement later.\n",
    "            internal_delta = pyro.sample(\n",
    "            \"internal_delta\",\n",
    "            dist.Normal(torch.zeros(2), torch.ones(2)).to_event(1).mask(False),\n",
    "            )  # [self.num_internal, 2]\n",
    "        baseline_0, baseline_1 = self.baseline\n",
    "        internal_location = baseline_0 + baseline_1 @ internal_delta\n",
    "\n",
    "        time = torch.zeros((self.num_nodes,))\n",
    "        time[self.is_internal] = internal_time\n",
    "        # GEOGRAPHY.\n",
    "        # Should we be Bayesian about migration scale, or should it be fixed?\n",
    "        migration_scale = pyro.sample(\"migration_scale\", dist.LogNormal(-5, 1))\n",
    "\n",
    "        # Next add a factor for time gaps between parents and children.\n",
    "        gap = time[self.parent] - time[self.child]  # [num_edges]\n",
    "\n",
    "        with pyro.plate(\"edges\", len(gap)):\n",
    "            if self.node_times is None:\n",
    "                rate = (gap * self.span * self.mutation_rate).clamp(min=1e-8)\n",
    "                pyro.sample(\n",
    "                \"mutations\",\n",
    "                dist.Poisson(rate),\n",
    "                obs=self.mutations,\n",
    "                )\n",
    "            else:\n",
    "                time = self.node_times\n",
    "            if self.leaf_location is not None:\n",
    "                gap = gap.clamp(min=1e-10)\n",
    "                # GEOGRAPHY.\n",
    "                # The following encodes that children migrate away from their parents\n",
    "                # following brownian motion with rate migration_scale.\n",
    "                location = torch.cat([self.leaf_location, internal_location], 0)\n",
    "                parent_location = location.index_select(0, self.parent)\n",
    "                child_location = location.index_select(0, self.child)\n",
    "\n",
    "                pyro.sample(\n",
    "                    \"migration\",\n",
    "                    tspyro.diffusion.WaypointDiffusion2D(\n",
    "                        source=parent_location,\n",
    "                        time=gap,\n",
    "                        radius=self.waypoint_radius,\n",
    "                        waypoints=self.waypoints,\n",
    "                        matrix_exp=self.matrix_exp,\n",
    "                    ),\n",
    "                    obs=child_location,\n",
    "                )\n",
    "            else:\n",
    "                location = torch.ones(self.ts.num_nodes)\n",
    "        return time, gap, location, migration_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "\n",
    "def radians_center_weighted(x, y, z, weights):\n",
    "    total_weight = np.sum(weights)\n",
    "    weighted_avg_x = np.sum(weights * np.array(x)) / total_weight\n",
    "    weighted_avg_y = np.sum(weights * np.array(y)) / total_weight\n",
    "    weighted_avg_z = np.sum(weights * np.array(z)) / total_weight\n",
    "    central_longitude = np.arctan2(weighted_avg_y, weighted_avg_x)\n",
    "    central_square_root = np.sqrt(\n",
    "        weighted_avg_x * weighted_avg_x + weighted_avg_y * weighted_avg_y\n",
    "    )\n",
    "    central_latitude = np.arctan2(weighted_avg_z, central_square_root)\n",
    "    return central_latitude, central_longitude\n",
    "\n",
    "\n",
    "def vectorized_weighted_geographic_center(lat_arr, long_arr, weights):\n",
    "    lat_arr = np.radians(lat_arr)\n",
    "    long_arr = np.radians(long_arr)\n",
    "    x = np.cos(lat_arr) * np.cos(long_arr)\n",
    "    y = np.cos(lat_arr) * np.sin(long_arr)\n",
    "    z = np.sin(lat_arr)\n",
    "    if len(weights.shape) > 1:\n",
    "        total_weights = np.sum(weights, axis=1)\n",
    "    else:\n",
    "        total_weights = np.sum(weights)\n",
    "    weighted_avg_x = np.sum(weights * x, axis=1) / total_weights\n",
    "    weighted_avg_y = np.sum(weights * y, axis=1) / total_weights\n",
    "    weighted_avg_z = np.sum(weights * z, axis=1) / total_weights\n",
    "    central_longitude = np.arctan2(weighted_avg_y, weighted_avg_x)\n",
    "    central_sqrt = np.sqrt((weighted_avg_x ** 2) + (weighted_avg_y ** 2))\n",
    "    central_latitude = np.arctan2(weighted_avg_z, central_sqrt)\n",
    "    return np.degrees(central_latitude), np.degrees(central_longitude)\n",
    "\n",
    "def weighted_geographic_center(lat_list, long_list, weights):\n",
    "    x = list()\n",
    "    y = list()\n",
    "    z = list()\n",
    "    if len(lat_list) == 1 and len(long_list) == 1:\n",
    "        return (lat_list[0], long_list[0])\n",
    "    lat_radians = np.radians(lat_list)\n",
    "    long_radians = np.radians(long_list)\n",
    "    x = np.cos(lat_radians) * np.cos(long_radians)\n",
    "    y = np.cos(lat_radians) * np.sin(long_radians)\n",
    "    z = np.sin(lat_radians)\n",
    "    weights = np.array(weights)\n",
    "    central_latitude, central_longitude = radians_center_weighted(x, y, z, weights)\n",
    "    return (np.degrees(central_latitude), np.degrees(central_longitude))\n",
    "\n",
    "def get_parent_age(self, edge):\n",
    "    times = self.ts.tables.nodes.time[:]\n",
    "    return times[operator.attrgetter(\"parent\")(edge)]\n",
    "\n",
    "def edges_by_parent_age_asc(self):\n",
    "    return itertools.groupby(self.ts.edges(), self.get_parent_age)\n",
    "\n",
    "def edges_by_parent_asc(ts):\n",
    "    \"\"\"\n",
    "    Return an itertools.groupby object of edges grouped by parent in ascending order\n",
    "    of the time of the parent. Since tree sequence properties guarantee that edges\n",
    "    are listed in nondecreasing order of parent time\n",
    "    (https://tskit.readthedocs.io/en/latest/data-model.html#edge-requirements)\n",
    "    we can simply use the standard edge order\n",
    "    \"\"\"\n",
    "    return itertools.groupby(ts.edges(), operator.attrgetter(\"parent\"))\n",
    "\n",
    "def parents_in_epoch(parents):\n",
    "    return itertools.groupby(parents, operator.attrgetter(\"parent\"))\n",
    "\n",
    "def edge_span(edge):\n",
    "    return edge.right - edge.left\n",
    "\n",
    "def average_edges(parent_edges, locations):\n",
    "    parent = parent_edges[0]\n",
    "    edges = parent_edges[1]\n",
    "\n",
    "    child_spanfracs = list()\n",
    "    child_lats = list()\n",
    "    child_longs = list()\n",
    "\n",
    "    for edge in edges:\n",
    "        child_spanfracs.append(edge_span(edge))\n",
    "        child_lats.append(locations[edge.child][0])\n",
    "        child_longs.append(locations[edge.child][1])\n",
    "    val = weighted_geographic_center(\n",
    "        child_lats, child_longs, np.ones_like(len(child_lats))\n",
    "    )\n",
    "    return parent, val\n",
    "\n",
    "def get_ancestral_geography(ts, sample_locations, show_progress=False):\n",
    "  \"\"\"\n",
    "  Use dynamic programming to find approximate posterior to sample from\n",
    "  \"\"\"\n",
    "  locations = np.zeros((ts.num_nodes, 2))\n",
    "  locations[ts.samples()] = sample_locations\n",
    "  fixed_nodes = set(ts.samples())\n",
    "\n",
    "  # Iterate through the nodes via groupby on parent node\n",
    "  for parent_edges in edges_by_parent_asc(ts):\n",
    "      if parent_edges[0] not in fixed_nodes:\n",
    "          parent, val = average_edges(parent_edges, locations)\n",
    "          locations[parent] = val\n",
    "  return torch.tensor(locations[ts.num_samples:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import init_to_median\n",
    "\n",
    "def date_pyro(ts, prior, transitions, waypoints, waypoint_radius,\n",
    "              leaf_location=None, node_times=None,\n",
    "              Ne=10000, mutation_rate=1e-8, steps=1001,\n",
    "              Model=ReparamModel):\n",
    "\n",
    "    pyro.set_rng_seed(20210518)\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    model = Model(ts, transitions=transitions, waypoints=waypoints, waypoint_radius=waypoint_radius,\n",
    "                 leaf_location=leaf_location, node_times=node_times, prior=prior, Ne=Ne, mutation_rate=mutation_rate)\n",
    "\n",
    "    # autoguide initialises location param, but we want a different initialiser for each pyro sample statement\n",
    "    # if we wanted to initialise global params, then check 'if site[\"name\"] == \"global_param_name\":'\n",
    "    # (this is a property of the approx posterior, not model, hence it's here and not in forward())\n",
    "    def init_loc_fn(site):\n",
    "        if site[\"name\"] == \"internal_time\":\n",
    "            prior_init = np.einsum(\"t,nt->n\", priors.timepoints, (priors.grid_data[:]))/np.sum(priors.grid_data[:], axis=1)\n",
    "            internal_time = torch.as_tensor(prior_init, dtype=torch.float) #/ Ne\n",
    "            return internal_time.clamp(min=0.1)\n",
    "        # GEOGRAPHY.\n",
    "        if site[\"name\"] == \"internal_location\":\n",
    "            initial_guess_loc = get_ancestral_geography(ts, leaf_location)#np.repeat([[10,10]], sample_ts.num_samples, axis=0))\n",
    "            return initial_guess_loc\n",
    "        return init_to_median(site) # automatic strategy (the default) if not \"internal_time\"\n",
    "\n",
    "    # guide = AutoDelta(model) # Simple point estimate\n",
    "    guide = AutoNormal(model, init_scale=0.01, init_loc_fn=init_loc_fn) # Mean field (fully Bayesian)\n",
    "    # guide = pyro.infer.autoguide.AutoLowRankMultivariateNormal(model, init_scale=0.01)#, init_loc_fn=init_loc_fn) # bigger matricies means clipped adam works better\n",
    "    # optim = Adam({\"lr\": 0.05})\n",
    "    optim = pyro.optim.ClippedAdam({\"lr\": 0.02, \"lrd\": 0.1**(1/max(1, steps))}) # lrd (learning rate decay) decreases learning rate over num of steps from lr to lr * 0.01\n",
    "    # optim = pyro.optim.ClippedAdam({\"lr\": 0.005})\n",
    "    svi = SVI(model, guide, optim, Trace_ELBO())\n",
    "    guide() # initialises the guide\n",
    "    losses = []\n",
    "    migration_scales = []\n",
    "    for step in range(steps):\n",
    "        loss = svi.step() / ts.num_nodes\n",
    "        losses.append(loss)\n",
    "        if step % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                median = guide.median() # assess convergence of migration scale parameter\n",
    "                migration_scale = float(median[\"migration_scale\"])\n",
    "                migration_scales.append(migration_scale)\n",
    "            print(f\"step {step} loss = {loss:0.5g}, Migration scale= {migration_scale:0.3g}\")\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(6,2))\n",
    "    # plt.figure(figsize=(6,2))\n",
    "    ax[0].plot(losses)\n",
    "    ax[0].set_xlabel(\"SVI step\")\n",
    "    ax[0].set_ylabel(\"loss\")\n",
    "    ax[0].set_yscale(\"symlog\")\n",
    "    ax[1].plot(migration_scales)\n",
    "    median = guide.median()\n",
    "    pyro_time, gaps, location, migration_scale = poutine.condition(model, median)()\n",
    "    return model, pyro_time, gaps, location, migration_scale, guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = tsdate.build_prior_grid(recap_ts, Ne=10000, approximate_priors=True, timepoints=100, progress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step is noticeably much slower when using waypoints, and the migration scale seems to grow monotonically. While the loss function decreases, and the results are worse than using the average of the child locations, is this something about the training rate? Or is something not parameterised correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mutation_rate=1e-7\n",
    "node_times = torch.as_tensor(recap_ts.tables.nodes.time, dtype=torch.float)\n",
    "leaf_location=torch.as_tensor(lat_long[:recap_ts.num_samples,:], dtype=torch.float)\n",
    "model, pyro_dates, gaps, location, migration_scale, guide = date_pyro(\n",
    "    recap_ts, priors, transition, waypoints, 0.2,\n",
    "    leaf_location=leaf_location,\n",
    "    node_times=node_times,\n",
    "    Ne=1000,\n",
    "    steps=200, mutation_rate=mutation_rate,\n",
    "    Model=ReparamModel,\n",
    ")\n",
    "                                              \n",
    "# prior_dates, _, location_prior = model() # run the model to sample from the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_location = get_ancestral_geography(recap_ts, lat_long[:recap_ts.num_samples])\n",
    "average_location = np.concatenate([lat_long[:recap_ts.num_samples], average_location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Node locations\n",
    "real_locations_internal = lat_long[recap_ts.num_samples:,]\n",
    "average_locations_internal = average_location[recap_ts.num_samples:,]\n",
    "inferred_locations_internal = location[recap_ts.num_samples:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(real_locations_internal[:,0], real_locations_internal[:,1], s=0.1)\n",
    "plt.scatter(average_locations_internal[:,0], average_locations_internal[:,1], s=0.1)\n",
    "# for real, predicted in zip(real_locations_internal, average_locations_internal):\n",
    "#     plt.plot((real[0], predicted[0]), (real[1], predicted[1]), linewidth=0.1, color=\"grey\")\n",
    "# plt.title(\"Real vs. Predicted Locations: Average of Children Locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    real_locations_internal[:,0], real_locations_internal[:,1], s=0.1, c=\"b\", label=\"True Location\")\n",
    "plt.scatter(\n",
    "    inferred_locations_internal[:,0],\n",
    "    inferred_locations_internal[:,1], s=0.1, c=\"orange\", label=\"Inferred Location\")\n",
    "X, Y = [], []\n",
    "for real, predicted in zip(real_locations_internal.tolist(), inferred_locations_internal.tolist()):\n",
    "    X.extend([real[0], predicted[0], None])\n",
    "    Y.extend([real[1], predicted[1], None])\n",
    "plt.plot(X, Y, linewidth=0.1, color=\"grey\")\n",
    "\n",
    "# Overlay the waypoints\n",
    "plt.scatter(waypoints[:,0], waypoints[:,1], zorder=-1, alpha=0.1, color=\"grey\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "ax[0].scatter(recap_ts.tables.nodes.time[:len(lat_long)], lat_long[:,0], s=1)\n",
    "ax[0].scatter(recap_ts.tables.nodes.time[:len(lat_long)], average_location[:len(lat_long),0], s=1)\n",
    "ax[1].scatter(recap_ts.tables.nodes.time[:len(lat_long)], lat_long[:,1], s=1)\n",
    "ax[1].scatter(recap_ts.tables.nodes.time[:len(lat_long)], average_location[:len(lat_long),1], s=1)\n",
    "fig.suptitle(\"Average of Children: Time vs. Lat/Long\")\n",
    "ax[0].set_xscale(\"symlog\")\n",
    "ax[1].set_xscale(\"symlog\")\n",
    "plt.xlabel(\"Time\")\n",
    "ax[0].set_ylabel(\"Latitude\")\n",
    "ax[1].set_ylabel(\"Longitude\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "ax[0].scatter(recap_ts.tables.nodes.time[:len(lat_long)], lat_long[:,0], s=1)\n",
    "ax[0].scatter(recap_ts.tables.nodes.time[:len(lat_long)], location[:len(lat_long),0], s=1)\n",
    "ax[1].scatter(recap_ts.tables.nodes.time[:len(lat_long)], lat_long[:,1], s=1)\n",
    "ax[1].scatter(recap_ts.tables.nodes.time[:len(lat_long)], location[:len(lat_long),1], s=1)\n",
    "fig.suptitle(\"tspyro: Time vs. Lat/Long\")\n",
    "ax[0].set_xscale(\"symlog\")\n",
    "ax[1].set_xscale(\"symlog\")\n",
    "plt.xlabel(\"Time\")\n",
    "ax[0].set_ylabel(\"Longitude\")\n",
    "ax[1].set_ylabel(\"Latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_location_scaled = sklearn.preprocessing.minmax_scale(average_location, feature_range=(0,1))\n",
    "real_location_scaled = sklearn.preprocessing.minmax_scale(lat_long, feature_range=(0,1))\n",
    "location_scaled = sklearn.preprocessing.minmax_scale(location, feature_range=(0,1))\n",
    "print(\"Pyro Model:\", mean_squared_error(real_location_scaled,\n",
    "                                        location_scaled[:len(lat_long)]))\n",
    "print(\"Average of children model:\", mean_squared_error(real_location_scaled,\n",
    "                                                       average_location_scaled[:len(lat_long)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yan's empirical dispersal rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.individual(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tskit\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ts = tskit.load(\"spatial_sim.trees\")\n",
    "distances = collections.defaultdict(list)\n",
    "locations = ts.tables.individuals.location.reshape(-1, 3)\n",
    "for e in ts.edges():\n",
    "    parent = ts.node(e.parent)\n",
    "    child = ts.node(e.child)\n",
    "#     distances[parent.time - child.time].append(np.sqrt(np.sum(locations[parent.individual, 0:2] ** 2)))\n",
    "    distances[parent.time - child.time].append(\n",
    "        np.sqrt(np.sum((locations[parent.individual, 0:2] - locations[child.individual, 0:2]) ** 2)))\n",
    "\n",
    "    \n",
    "# turn into log times, and don't show the really long branches (large time deltas)\n",
    "distances = {np.log(time):v for time, v in distances.items() if time < 40}\n",
    "\n",
    "max_distance = max(max(d) for d in distances.values())\n",
    "bins = np.linspace(0, max_distance, 20)\n",
    "h = np.array([np.histogram(v, bins=bins, density=True)[0] for v in distances.values()])\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(2, figsize=(10, 10))\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "ax0.pcolormesh(\n",
    "    [0] + [k for k in distances.keys()],\n",
    "    bins,\n",
    "    h.T)\n",
    "ax0.set_xlabel(\"Log time\")\n",
    "ax0.set_ylabel(\"Distance\")\n",
    "smallest_log_dist = min(distances.keys())\n",
    "ax1.hist(distances[smallest_log_dist], density=True, bins=50)\n",
    "ax1.set_title(f\"Histogram based on edges of branch length {np.exp(smallest_log_dist)}\")\n",
    "ax1.set_xlabel(\"Parent-child geographical distance\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import SVG\n",
    "SVG(ts_simp.first().draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_simp = ts.simplify(np.arange(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way is to save all the individuals, so every edge should have be of length 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
